{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear regression from scratch\n",
    "\n",
    "- Prediction: Manually\n",
    "- Gradient Computation: Manually\n",
    "- Loss Computation: Manually\n",
    "- Parameter update: Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.0\n",
      "epoch: 0, w = 1.200, loss = 30.00000000\n",
      "epoch: 1, w = 1.680, loss = 4.80000067\n",
      "epoch: 2, w = 1.872, loss = 0.76800019\n",
      "epoch: 3, w = 1.949, loss = 0.12288000\n",
      "epoch: 4, w = 1.980, loss = 0.01966083\n",
      "epoch: 5, w = 1.992, loss = 0.00314574\n",
      "epoch: 6, w = 1.997, loss = 0.00050332\n",
      "epoch: 7, w = 1.999, loss = 0.00008053\n",
      "epoch: 8, w = 1.999, loss = 0.00001288\n",
      "epoch: 9, w = 2.000, loss = 0.00000206\n",
      "prediction after training: f(5) = 9.99895191192627\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1,2,3,4], dtype=np.float32)\n",
    "Y = np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "W = 0.0\n",
    "\n",
    "def forward(x,w):\n",
    "    return w * x\n",
    "\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "#MSE 1/N * (wx - y)**2\n",
    "#dJ/dw = 1/N 2 x (wx-y)\n",
    "\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print('prediction before training: f(5) = {}'.format(forward(5,W)))\n",
    "\n",
    "lr = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(X,W)\n",
    "    l = loss(Y, y_pred)\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    W -= lr*dw\n",
    "    print(\"epoch: {}, w = {:.3f}, loss = {:.8f}\".format(epoch, W, l))\n",
    "\n",
    "print('prediction after training: f(5) = {}'.format(forward(5,W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Prediction: Manually\n",
    "- Gradient Computation: Autograd\n",
    "- Loss Computation: Manually\n",
    "- Parameter update: Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.0\n",
      "epoch: 0, w = 0.300, loss = 30.00000000\n",
      "epoch: 1, w = 0.555, loss = 21.67499924\n",
      "epoch: 2, w = 0.772, loss = 15.66018772\n",
      "epoch: 3, w = 0.956, loss = 11.31448650\n",
      "epoch: 4, w = 1.113, loss = 8.17471695\n",
      "epoch: 5, w = 1.246, loss = 5.90623236\n",
      "epoch: 6, w = 1.359, loss = 4.26725292\n",
      "epoch: 7, w = 1.455, loss = 3.08308983\n",
      "epoch: 8, w = 1.537, loss = 2.22753215\n",
      "epoch: 9, w = 1.606, loss = 1.60939169\n",
      "epoch: 10, w = 1.665, loss = 1.16278565\n",
      "epoch: 11, w = 1.716, loss = 0.84011245\n",
      "epoch: 12, w = 1.758, loss = 0.60698116\n",
      "epoch: 13, w = 1.794, loss = 0.43854395\n",
      "epoch: 14, w = 1.825, loss = 0.31684780\n",
      "epoch: 15, w = 1.851, loss = 0.22892261\n",
      "epoch: 16, w = 1.874, loss = 0.16539653\n",
      "epoch: 17, w = 1.893, loss = 0.11949898\n",
      "epoch: 18, w = 1.909, loss = 0.08633806\n",
      "epoch: 19, w = 1.922, loss = 0.06237914\n",
      "prediction after training: f(5) = 9.612405776977539\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "W = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x,w):\n",
    "    return w * x\n",
    "\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "print('prediction before training: f(5) = {}'.format(forward(5,W)))\n",
    "\n",
    "lr = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(X,W)\n",
    "    l = loss(Y, y_pred)\n",
    "    l.backward()\n",
    "\n",
    "    with torch.no_grad(): #equal to optimizer.step\n",
    "        W -= lr* W.grad #optimization\n",
    "\n",
    "    W.grad.zero_()\n",
    "\n",
    "\n",
    "    print(\"epoch: {}, w = {:.3f}, loss = {:.8f}\".format(epoch, W, l))\n",
    "\n",
    "print('prediction after training: f(5) = {}'.format(forward(5,W)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Prediction: Manually\n",
    "- Gradient Computation: Autograd\n",
    "- Loss Computation: pytorch loss\n",
    "- Parameter update: pytorch optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Design the model\n",
    "    * Input size\n",
    "    * Output size\n",
    "    * Forward pass\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop\n",
    "    * Forward pass: Compute the prediction\n",
    "    * Backward pass: Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Prediction: Manually\n",
    "- Gradient Computation: Autograd\n",
    "- Loss Computation: Pytorch MSELoss\n",
    "- Parameter update: Pytorch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, w=0.300, loss=30.00000000\n",
      "epoch: 1, w=0.555, loss=21.67499924\n",
      "epoch: 2, w=0.772, loss=15.66018772\n",
      "epoch: 3, w=0.956, loss=11.31448650\n",
      "epoch: 4, w=1.113, loss=8.17471695\n",
      "epoch: 5, w=1.246, loss=5.90623236\n",
      "epoch: 6, w=1.359, loss=4.26725292\n",
      "epoch: 7, w=1.455, loss=3.08308983\n",
      "epoch: 8, w=1.537, loss=2.22753215\n",
      "epoch: 9, w=1.606, loss=1.60939169\n",
      "epoch: 10, w=1.665, loss=1.16278565\n",
      "epoch: 11, w=1.716, loss=0.84011245\n",
      "epoch: 12, w=1.758, loss=0.60698116\n",
      "epoch: 13, w=1.794, loss=0.43854395\n",
      "epoch: 14, w=1.825, loss=0.31684780\n",
      "epoch: 15, w=1.851, loss=0.22892261\n",
      "epoch: 16, w=1.874, loss=0.16539653\n",
      "epoch: 17, w=1.893, loss=0.11949898\n",
      "epoch: 18, w=1.909, loss=0.08633806\n",
      "epoch: 19, w=1.922, loss=0.06237914\n",
      "prediction after training: f(5) = 9.612405776977539\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "W = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "iters = 20\n",
    "lr = 0.01\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([W],lr=lr) #argument - parameter and learning rate\n",
    "\n",
    "def forward(x, w):\n",
    "    return w*x\n",
    "\n",
    "\n",
    "for epoch in range(iters):\n",
    "    y_pred = forward(X, W)\n",
    "    l = loss(y_pred, Y)\n",
    "\n",
    "    l.backward() #calculate gradient\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    W.grad.zero_()\n",
    "\n",
    "    print(\"epoch: {}, w={:.3f}, loss={:.8f}\".format(epoch, W, l))\n",
    "\n",
    "\n",
    "print('prediction after training: f(5) = {}'.format(forward(5,W)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Prediction: nn.Linear\n",
    "- Gradient Computation: Autograd\n",
    "- Loss Computation: Pytorch MSELoss\n",
    "- Parameter update: Pytorch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "prediction before training f(5)=-3.741501808166504\n",
      "epoch: 0, loss: 55.85092544555664 w=-0.36824142932891846, b=0.28225255012512207\n",
      "epoch: 1, loss: 38.80171203613281 w=-0.027117855846881866, b=0.3950195610523224\n",
      "epoch: 2, loss: 26.971336364746094 w=0.25719884037971497, b=0.4884750545024872\n",
      "epoch: 3, loss: 18.7622013092041 w=0.49419525265693665, b=0.5658456087112427\n",
      "epoch: 4, loss: 13.065776824951172 w=0.6917736530303955, b=0.6298189163208008\n",
      "epoch: 5, loss: 9.112865447998047 w=0.8565166592597961, b=0.6826338768005371\n",
      "epoch: 6, loss: 6.369742393493652 w=0.9939074516296387, b=0.7261553406715393\n",
      "epoch: 7, loss: 4.466071128845215 w=1.108513593673706, b=0.7619368433952332\n",
      "epoch: 8, loss: 3.144876480102539 w=1.2041397094726562, b=0.7912724018096924\n",
      "epoch: 9, loss: 2.2278530597686768 w=1.2839550971984863, b=0.8152399659156799\n",
      "epoch: 10, loss: 1.5912765264511108 w=1.3505998849868774, b=0.8347374200820923\n",
      "epoch: 11, loss: 1.1492975950241089 w=1.4062730073928833, b=0.850512683391571\n",
      "epoch: 12, loss: 0.8423483371734619 w=1.4528064727783203, b=0.8631888031959534\n",
      "epoch: 13, loss: 0.6290940046310425 w=1.491726040840149, b=0.8732846975326538\n",
      "epoch: 14, loss: 0.4808547794818878 w=1.5243028402328491, b=0.8812326788902283\n",
      "epoch: 15, loss: 0.37773001194000244 w=1.5515958070755005, b=0.8873928785324097\n",
      "epoch: 16, loss: 0.30590999126434326 w=1.5744867324829102, b=0.8920652270317078\n",
      "epoch: 17, loss: 0.2558140754699707 w=1.5937104225158691, b=0.895499587059021\n",
      "epoch: 18, loss: 0.22079288959503174 w=1.6098788976669312, b=0.897904098033905\n",
      "epoch: 19, loss: 0.19623374938964844 w=1.6235018968582153, b=0.8994520902633667\n",
      "epoch: 20, loss: 0.17893512547016144 w=1.6350040435791016, b=0.900287926197052\n",
      "epoch: 21, loss: 0.1666763722896576 w=1.644739031791687, b=0.9005319476127625\n",
      "epoch: 22, loss: 0.15791624784469604 w=1.6530015468597412, b=0.9002843499183655\n",
      "epoch: 23, loss: 0.15158499777317047 w=1.6600371599197388, b=0.8996285796165466\n",
      "epoch: 24, loss: 0.14694058895111084 w=1.6660501956939697, b=0.898634135723114\n",
      "epoch: 25, loss: 0.14346840977668762 w=1.6712110042572021, b=0.8973589539527893\n",
      "epoch: 26, loss: 0.14081087708473206 w=1.6756614446640015, b=0.8958512544631958\n",
      "epoch: 27, loss: 0.13872024416923523 w=1.6795196533203125, b=0.8941511511802673\n",
      "epoch: 28, loss: 0.13702432811260223 w=1.6828840970993042, b=0.8922921419143677\n",
      "epoch: 29, loss: 0.1356038898229599 w=1.685836911201477, b=0.8903021216392517\n",
      "epoch: 30, loss: 0.13437584042549133 w=1.688446283340454, b=0.8882042169570923\n",
      "epoch: 31, loss: 0.13328301906585693 w=1.690769076347351, b=0.8860177993774414\n",
      "epoch: 32, loss: 0.13228532671928406 w=1.6928528547286987, b=0.8837589621543884\n",
      "epoch: 33, loss: 0.1313548982143402 w=1.6947369575500488, b=0.8814411163330078\n",
      "epoch: 34, loss: 0.13047289848327637 w=1.696454405784607, b=0.8790754675865173\n",
      "epoch: 35, loss: 0.12962566316127777 w=1.6980324983596802, b=0.8766712546348572\n",
      "epoch: 36, loss: 0.12880408763885498 w=1.6994940042495728, b=0.8742362260818481\n",
      "epoch: 37, loss: 0.12800177931785583 w=1.7008581161499023, b=0.871776819229126\n",
      "epoch: 38, loss: 0.12721404433250427 w=1.7021405696868896, b=0.8692983984947205\n",
      "epoch: 39, loss: 0.12643788754940033 w=1.7033545970916748, b=0.866805374622345\n",
      "epoch: 40, loss: 0.12567110359668732 w=1.7045111656188965, b=0.8643015623092651\n",
      "epoch: 41, loss: 0.12491229921579361 w=1.70561945438385, b=0.8617899417877197\n",
      "epoch: 42, loss: 0.12416023015975952 w=1.706687092781067, b=0.8592731952667236\n",
      "epoch: 43, loss: 0.12341427803039551 w=1.707720398902893, b=0.8567533493041992\n",
      "epoch: 44, loss: 0.12267383933067322 w=1.7087246179580688, b=0.8542322516441345\n",
      "epoch: 45, loss: 0.12193866819143295 w=1.7097042798995972, b=0.8517113924026489\n",
      "epoch: 46, loss: 0.12120851129293442 w=1.710663080215454, b=0.8491919636726379\n",
      "epoch: 47, loss: 0.12048293650150299 w=1.7116039991378784, b=0.8466749787330627\n",
      "epoch: 48, loss: 0.11976208537817001 w=1.7125296592712402, b=0.8441612720489502\n",
      "epoch: 49, loss: 0.11904558539390564 w=1.7134422063827515, b=0.8416515588760376\n",
      "epoch: 50, loss: 0.11833358556032181 w=1.7143433094024658, b=0.8391464352607727\n",
      "epoch: 51, loss: 0.11762583255767822 w=1.7152345180511475, b=0.836646318435669\n",
      "epoch: 52, loss: 0.116922527551651 w=1.716117024421692, b=0.8341516852378845\n",
      "epoch: 53, loss: 0.11622325330972672 w=1.716991901397705, b=0.8316627740859985\n",
      "epoch: 54, loss: 0.11552830785512924 w=1.7178599834442139, b=0.8291799426078796\n",
      "epoch: 55, loss: 0.11483760178089142 w=1.7187219858169556, b=0.8267033696174622\n",
      "epoch: 56, loss: 0.11415094882249832 w=1.719578504562378, b=0.8242331743240356\n",
      "epoch: 57, loss: 0.1134684756398201 w=1.7204301357269287, b=0.8217695951461792\n",
      "epoch: 58, loss: 0.11278997361660004 w=1.721277117729187, b=0.8193126916885376\n",
      "epoch: 59, loss: 0.11211568117141724 w=1.722119927406311, b=0.8168625831604004\n",
      "epoch: 60, loss: 0.11144531518220901 w=1.7229588031768799, b=0.8144193291664124\n",
      "epoch: 61, loss: 0.11077898740768433 w=1.7237939834594727, b=0.8119829893112183\n",
      "epoch: 62, loss: 0.11011669784784317 w=1.7246257066726685, b=0.8095536231994629\n",
      "epoch: 63, loss: 0.10945829004049301 w=1.7254542112350464, b=0.807131290435791\n",
      "epoch: 64, loss: 0.10880386829376221 w=1.7262794971466064, b=0.8047159314155579\n",
      "epoch: 65, loss: 0.1081533133983612 w=1.7271018028259277, b=0.8023076057434082\n",
      "epoch: 66, loss: 0.10750669240951538 w=1.7279211282730103, b=0.7999063730239868\n",
      "epoch: 67, loss: 0.10686396807432175 w=1.7287375926971436, b=0.7975121736526489\n",
      "epoch: 68, loss: 0.10622504353523254 w=1.7295513153076172, b=0.7951250672340393\n",
      "epoch: 69, loss: 0.10558989644050598 w=1.7303624153137207, b=0.7927449941635132\n",
      "epoch: 70, loss: 0.10495863109827042 w=1.7311707735061646, b=0.7903719544410706\n",
      "epoch: 71, loss: 0.1043311133980751 w=1.7319765090942383, b=0.7880059480667114\n",
      "epoch: 72, loss: 0.10370725393295288 w=1.7327797412872314, b=0.7856470346450806\n",
      "epoch: 73, loss: 0.10308727622032166 w=1.733580470085144, b=0.7832950949668884\n",
      "epoch: 74, loss: 0.10247085243463516 w=1.734378695487976, b=0.7809501886367798\n",
      "epoch: 75, loss: 0.10185827314853668 w=1.7351744174957275, b=0.7786122560501099\n",
      "epoch: 76, loss: 0.10124927759170532 w=1.7359676361083984, b=0.7762812972068787\n",
      "epoch: 77, loss: 0.10064390301704407 w=1.7367584705352783, b=0.7739573121070862\n",
      "epoch: 78, loss: 0.10004214942455292 w=1.7375468015670776, b=0.7716402411460876\n",
      "epoch: 79, loss: 0.09944397956132889 w=1.738332748413086, b=0.7693300843238831\n",
      "epoch: 80, loss: 0.09884944558143616 w=1.7391163110733032, b=0.7670268416404724\n",
      "epoch: 81, loss: 0.09825840592384338 w=1.7398974895477295, b=0.7647305130958557\n",
      "epoch: 82, loss: 0.09767098724842072 w=1.7406764030456543, b=0.7624410390853882\n",
      "epoch: 83, loss: 0.09708705544471741 w=1.741452932357788, b=0.7601584196090698\n",
      "epoch: 84, loss: 0.09650653600692749 w=1.7422270774841309, b=0.7578825950622559\n",
      "epoch: 85, loss: 0.09592956304550171 w=1.7429988384246826, b=0.7556135654449463\n",
      "epoch: 86, loss: 0.09535608440637589 w=1.743768334388733, b=0.7533513307571411\n",
      "epoch: 87, loss: 0.09478592872619629 w=1.7445355653762817, b=0.7510958909988403\n",
      "epoch: 88, loss: 0.09421921521425247 w=1.7453004121780396, b=0.7488471865653992\n",
      "epoch: 89, loss: 0.09365588426589966 w=1.746062994003296, b=0.7466052174568176\n",
      "epoch: 90, loss: 0.09309589117765427 w=1.7468233108520508, b=0.7443699836730957\n",
      "epoch: 91, loss: 0.09253930300474167 w=1.7475813627243042, b=0.7421414256095886\n",
      "epoch: 92, loss: 0.0919860377907753 w=1.7483371496200562, b=0.7399195432662964\n",
      "epoch: 93, loss: 0.09143601357936859 w=1.749090552330017, b=0.7377042770385742\n",
      "epoch: 94, loss: 0.09088943153619766 w=1.7498418092727661, b=0.7354956865310669\n",
      "epoch: 95, loss: 0.09034599363803864 w=1.7505908012390137, b=0.7332937121391296\n",
      "epoch: 96, loss: 0.08980581909418106 w=1.7513375282287598, b=0.7310982942581177\n",
      "epoch: 97, loss: 0.08926883339881897 w=1.7520819902420044, b=0.728909432888031\n",
      "epoch: 98, loss: 0.08873508870601654 w=1.7528241872787476, b=0.7267271280288696\n",
      "epoch: 99, loss: 0.08820462226867676 w=1.7535642385482788, b=0.7245513796806335\n",
      "prediction before training f(5)=9.492372512817383\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "print(X.shape)\n",
    "\n",
    "iters = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# model = nn.Linear(in_features=X.shape[1], out_features=1)\n",
    "#or using class wraper\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(X.shape[1], 1)\n",
    "\n",
    "\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "print(\"prediction before training f(5)={}\".format(model(X_test).item()))\n",
    "\n",
    "\n",
    "for epoch in range(iters):\n",
    "    Y_pred = model(X)\n",
    "    l = loss(Y, Y_pred)\n",
    "\n",
    "    l.backward()\n",
    "\n",
    "    optimizer.step() #update parameter using gradient\n",
    "    optimizer.zero_grad() #clear gradients\n",
    "    w,b = model.parameters() #unpack parameter of linear layer\n",
    "\n",
    "    print(\"epoch: {}, loss: {} w={}, b={}\".format(epoch, l, w.item(), b.item()))\n",
    "\n",
    "\n",
    "\n",
    "print(\"prediction before training f(5)={}\".format(model(X_test).item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.3701]], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
