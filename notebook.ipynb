{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXmxlZtHstvg",
        "outputId": "e22abdb0-90a8-4386-958f-0f91abbd28b0"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from argparse import Namespace\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROX4Mfpsstvm"
      },
      "outputs": [],
      "source": [
        "class Vocabulary(object):\n",
        "\n",
        "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
        "\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self._token_to_idx = token_to_idx\n",
        "        self._idx_to_token = {idx:token for token, idx in self._token_to_idx.items()}\n",
        "\n",
        "        self._add_unk = add_unk\n",
        "        self._unk_token = unk_token\n",
        "\n",
        "        self.unk_index = -1\n",
        "        if add_unk:\n",
        "            self.unk_index = self.add_token(unk_token)\n",
        "\n",
        "    def to_serializable(self):\n",
        "\n",
        "        return {'token_to_idx':self._token_to_idx,\n",
        "                'add_unk':self._add_unk,\n",
        "                'unk_token':self._unk_token}\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "\n",
        "        if token in self._token_to_idx:\n",
        "            index = self._token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self._token_to_idx)\n",
        "            self._token_to_idx[token] = index\n",
        "            self._idx_to_token[index] = token\n",
        "\n",
        "        return index\n",
        "\n",
        "    def add_many(self, tokens):\n",
        "\n",
        "        return [self.add_token(token) for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        if self.unk_index >= 0:\n",
        "            return self._token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            return self._token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "\n",
        "        if index not in self._idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the vocabulary\" % index)\n",
        "        return self._idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._token_to_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfv_JkAsstvo"
      },
      "outputs": [],
      "source": [
        "class ReviewVectorizer(object):\n",
        "\n",
        "    def __init__(self, review_vocab, rating_vocab):\n",
        "\n",
        "        self.review_vocab = review_vocab\n",
        "        self.rating_vocab = rating_vocab\n",
        "\n",
        "    def vectorize(self, review):\n",
        "\n",
        "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
        "\n",
        "        for token in review.split(\" \"):\n",
        "            if token not in string.punctuation:\n",
        "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
        "\n",
        "        return one_hot\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, review_df, cutoff=25):\n",
        "        review_vocab = Vocabulary(add_unk=True)\n",
        "        rating_vocab = Vocabulary(add_unk=False)\n",
        "\n",
        "        for rating in sorted(set(review_df.rating)):\n",
        "            rating_vocab.add_token(rating)\n",
        "\n",
        "        word_count = Counter()\n",
        "        for review in review_df.review:\n",
        "            for word in review.split(\" \"):\n",
        "                if word not in string.punctuation:\n",
        "                    word_count[word] += 1\n",
        "\n",
        "        for word, count in word_count.items():\n",
        "            if count > cutoff:\n",
        "                review_vocab.add_token(word)\n",
        "\n",
        "        return cls(review_vocab, rating_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "\n",
        "        review_vocab = Vocabulary.from_serializable(contents[\"review_vocab\"])\n",
        "        rating_vocab = Vocabulary.from_serializable(contents[\"rating_vocab\"])\n",
        "\n",
        "        return cls(review_vocab, rating_vocab)\n",
        "\n",
        "    def to_serializable(self):\n",
        "\n",
        "        return {\"review_vocab\": self.review_vocab.to_serializable(),\n",
        "                \"rating_vocab\": self.rating_vocab.to_serializable()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFVjP8Phstvp"
      },
      "outputs": [],
      "source": [
        "class ReviewDataset(Dataset):\n",
        "\n",
        "    def __init__(self, review_df, vectorizer):\n",
        "        self.review_df = review_df\n",
        "        self._vectorizer = vectorizer\n",
        "\n",
        "        self.train_df = self.review_df[self.review_df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "\n",
        "        self.val_df = self.review_df[self.review_df.split=='val']\n",
        "        self.val_size = len(self.val_df)\n",
        "\n",
        "        self.test_df = self.review_df[self.review_df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "\n",
        "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
        "                             'val': (self.val_df, self.val_size),\n",
        "                             'test': ()}\n",
        "\n",
        "        self.set_split(\"train\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
        "        review_df = pd.read_csv(review_csv)\n",
        "        train_review_df = review_df[review_df.split==\"train\"]\n",
        "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath):\n",
        "        review_df = pd.read_csv(review_csv)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(review_df, vectorizer)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return ReviewVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self._vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def get_vectorizer(self):\n",
        "        return self._vectorizer\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        self._target_split = split\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._target_size\n",
        "\n",
        "    def __getitem__(self, index:int):\n",
        "        row = self._target_df.iloc[index]\n",
        "\n",
        "        review_vector = self._vectorizer.vectorize(row.review)\n",
        "        rating_index = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
        "\n",
        "        return{'x_data':review_vector,'y_target':rating_index}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return len(self) // batch_size\n",
        "\n",
        "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name].to(device)\n",
        "        yield out_data_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL3ISgz2stvr"
      },
      "outputs": [],
      "source": [
        "class ReviewClassifier(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(ReviewClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=num_features, out_features=1)\n",
        "\n",
        "    def forward(self, x_in, apply_sigmoid=False):\n",
        "        y_out = self.fc1(x_in).squeeze()\n",
        "        if apply_sigmoid:\n",
        "            y_out = torch.sigmoid(y_out)\n",
        "        return y_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MAPYCRfstvr"
      },
      "outputs": [],
      "source": [
        "def make_train_state(args):\n",
        "    return {'stop_early': False,\n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'learning_rate': args.learning_rate,\n",
        "            'epoch_index':0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': args.model_state_file}\n",
        "\n",
        "def update_train_state(args, model, train_state):\n",
        "    if train_state['epoch_index'] == 0:\n",
        "        torch.save(model.state_dict(), train_state['model_filename'])\n",
        "        train_state['stop_early'] = False\n",
        "    elif train_state['epoch_index'] >= 1:\n",
        "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
        "\n",
        "        if loss_t >= train_state['early_stopping_best_val']:\n",
        "            train_state['early_stopping_step'] += 1\n",
        "        else:\n",
        "            if loss_t < train_state['early_stopping_best_val']:\n",
        "                torch.save(model.state_dict(), train_state['model_filename'])\n",
        "\n",
        "            train_state['early_stopping_step'] = 0\n",
        "\n",
        "            train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
        "\n",
        "    return train_state\n",
        "\n",
        "def compute_accuracy(y_pred, y_target):\n",
        "    y_target = y_target.cpu()\n",
        "    y_pred_indices = (torch.sigmoid(y_pred) > 0.5).cpu().long()\n",
        "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "\n",
        "    return n_correct / len(y_pred_indices) * 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUQ2QiYFstvs"
      },
      "outputs": [],
      "source": [
        "def set_seed_everywhere(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def handle_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3n7SjyPstvt",
        "outputId": "8eebc024-7ff8-4643-f43f-fcf9501846c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expanded filepath: \n",
            "\t/home/hc/pytorch_book/vectorizer.json\n",
            "\t/home/hc/pytorch_book/model.pth\n",
            "Using CUDA: True\n"
          ]
        }
      ],
      "source": [
        "args = Namespace(\n",
        "    frequency_cutoff=25,\n",
        "    model_state_file='model.pth',\n",
        "    review_csv=\"reviews_with_splits_lite.csv\",\n",
        "    save_dir=Path().absolute(),\n",
        "    #save_dir=\"/home/hc/pytorch_book\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    batch_size=128,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=0.001,\n",
        "    num_epochs=100,\n",
        "    seed=1337,\n",
        "    catch_keyboard_interrupt=True,\n",
        "    cuda=True,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        "    reload_from_files=False,\n",
        "    )\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
        "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
        "\n",
        "    print(\"Expanded filepath: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhDva58mstvt",
        "outputId": "4ae9b7fd-fe71-4742-8a45-d65dee6aa27d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset and create vectorizer\n"
          ]
        }
      ],
      "source": [
        "if args.reload_from_files:\n",
        "    print(\"Loading dataset and vectorizer\")\n",
        "    dataset = ReviewDataset.load_dataset_and_load_vectorizer(args.review_csv, args.vectorizer_file)\n",
        "\n",
        "else:\n",
        "    print(\"Loading dataset and create vectorizer\")\n",
        "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "classifier = ReviewClassifier(len(vectorizer.review_vocab))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEjDIq0tstvu",
        "outputId": "ed1d952e-3026-4eb7-edf5-34ed4605f874"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7326,)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]['x_data'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhbbGrGBstvu",
        "outputId": "79a6f991-868c-4697-8d10-6784012fa794",
        "colab": {
          "referenced_widgets": [
            "bf5e05912d874b08a38a93520034d6a9",
            "26d0558615b9487aa6f2f1e5ed6591a1",
            "4075432cab394a2280d1f8027f72ab52"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf5e05912d874b08a38a93520034d6a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_routine:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26d0558615b9487aa6f2f1e5ed6591a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split=train:   0%|          | 0/306 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4075432cab394a2280d1f8027f72ab52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split=val:   0%|          | 0/65 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "classifier = classifier.to(args.device)\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(classifier.parameters())\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode=\"min\", factor=0.5, patience=1)\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "epoch_bar = tqdm(desc='training_routine',\n",
        "                          total=args.num_epochs,\n",
        "                          position=0)\n",
        "\n",
        "dataset.set_split(\"train\")\n",
        "train_bar = tqdm(desc='split=train',\n",
        "                          total=dataset.get_num_batches(args.batch_size),\n",
        "                          position=1,\n",
        "                          leave=True)\n",
        "dataset.set_split(\"val\")\n",
        "val_bar = tqdm(desc='split=val',\n",
        "                        total=dataset.get_num_batches(args.batch_size),\n",
        "                        position=1,\n",
        "                        leave=True)\n",
        "\n",
        "for epoch_index in range(args.num_epochs):\n",
        "    train_state['epoch_index'] = epoch_index\n",
        "\n",
        "    dataset.set_split('train')\n",
        "    batch_generator = generate_batches(dataset=dataset, batch_size=args.batch_size, device=args.device)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    classifier.train()\n",
        "\n",
        "    for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
        "\n",
        "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "        loss_t = loss.item()\n",
        "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "        train_bar.set_postfix(loss=running_loss,\n",
        "                              acc=running_acc,\n",
        "                              epoch=epoch_index)\n",
        "        train_bar.update()\n",
        "\n",
        "    train_state['train_loss'].append(running_loss)\n",
        "    train_state['train_acc'].append(running_acc)\n",
        "\n",
        "    dataset.set_split('val')\n",
        "    batch_generator = generate_batches(dataset=dataset, batch_size=args.batch_size, device=args.device)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    classifier.eval()\n",
        "\n",
        "    for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
        "\n",
        "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "        loss_t = loss.item()\n",
        "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'].float())\n",
        "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "        val_bar.set_postfix(loss=running_loss,\n",
        "                            acc=running_acc,\n",
        "                            epoch=epoch_index)\n",
        "        val_bar.update()\n",
        "\n",
        "    train_state['val_loss'].append(running_loss)\n",
        "    train_state['val_acc'].append(running_acc)\n",
        "\n",
        "    train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
        "\n",
        "    scheduler.step(train_state['val_loss'][-1])\n",
        "\n",
        "    train_bar.n = 0\n",
        "    val_bar.n = 0\n",
        "    epoch_bar.update()\n",
        "\n",
        "    if train_state['stop_early']:\n",
        "        break\n",
        "\n",
        "    train_bar.n = 0\n",
        "    val_bar.n = 0\n",
        "    epoch_bar.update()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orjaz4wsstvv"
      },
      "outputs": [],
      "source": [
        "batch_generator = generate_batches(dataset=dataset, batch_size=args.batch_size, device=args.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW87git6stvv",
        "outputId": "2eec0dc5-eaea-49e0-8ea4-5989cddbc85f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'x_data': tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [1., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
              " 'y_target': tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
              "         1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
              "         1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
              "         0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
              "         1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
              "         0, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(batch_generator)[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\""
      ],
      "metadata": {
        "id": "nzem7aLbswq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gutY9q6EswjR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "paper",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}